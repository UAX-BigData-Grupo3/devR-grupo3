---
title: "trabajo_grupal"
output:
  html_document: default
date: '2023-01-14'
---

## Trabajo grupal Programación para Big Data

### Autores:

-   Paula González Mataix
-   Cristian Sales Villa
-   Rafael Castillo García
-   Diego Cristóbal Herreros

#### Instalación de los paquetes requeridos

```{r dependencias, results = FALSE, echo=TRUE, message=FALSE, warning=FALSE}
if (!require(bookdown))
  install.packages("bookdown", dependencies = TRUE)
if (!require(readr))
  install.packages("readr")
if (!require(magrittr))
  install.packages("magrittr")
if (!require(dplyr))
  install.packages("dplyr") # alternative installation of the %>%
if (!require(tm))
  install.packages("tm")
if (!require(pdftools))
  install.packages("pdftools")
if (!require(caret))
  install.packages("caret", repos = "http://cran.us.r-project.org")
if (!require(data.table))
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if (!require(ggthemes))
  install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if (!require(scales))
  install.packages("scales", repos = "http://cran.us.r-project.org")
if (!require(gridExtra))
  install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if (!require(ggcorrplot))
  install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
if (!require(ggplot2))
  install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if (!require(rpart.plot))
  install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if (!require(readxl))
  install.packages("GGally")
if (!require(corrplot))
  install.packages("GGally")
if (!require(corrplot))
  install.packages("corrplot")
if (!require(PerformanceAnalytics))
  install.packages("PerformanceAnalytics")
if (!require(readxl))
  install.packages("readxl")
```

#### Carga de librerias

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
library(readr)
library(bookdown)
library(caret)
library(data.table)
library(grid)
library(gridExtra)
library(ggplot2)
library(ggcorrplot)
library(rpart)
library(readxl)
library(GGally)
library(corrplot)
library(PerformanceAnalytics)
library(tm)
library(pdftools)
library(readxl)
```

### Ejercicio 1: Preprocesamiento titanic

#### Creación del dataset

```{r load_data}
data_link <- "dataframe/"
df_train <- fread(paste0(data_link, "train.csv"), sep = ",", dec = ".")
df_test <- fread(paste0(data_link, "test.csv"), sep = ",", dec = ".")
titanic  <- dplyr::bind_rows(df_train, df_test) # bind training & test data
head(titanic, 10)
```

#### Chequeamos los datos

```{r show_data, echo=FALSE}
str(titanic)
```

#### Resumen estadístico

```{r stadistical_summary}
summary(titanic)
```

#### Búsqueda de nulos

```{r search_nulls}
titanic %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  t()
```

#### Buscamos si hay datos vacíos para estas variables

```{r check_nulls}
titanic %>%
  group_by(Sex) %>%
  filter(!is.na(Age)) %>%
  ggplot(aes(Age, y = ..count..)) +
  geom_density(alpha = 0.2, bw = 0.75, position = "stack") +
  facet_grid(Sex ~ .)
```

#### Comenzamos la limpieza de variables

```{r}
#Creamos un dataset para las transformaciones
titanic_transform <- data.frame(titanic)
head(titanic_transform, 10)
```

**Age**

```{r}
#Comprobamos si tenemos valores nulos y vemos
#que tenemos 418 en Survived y 263 en Age
colSums(is.na(titanic_transform))
```

```{r}
#Visualizamos los pasajeros que tienen nulos en la Age
head(titanic_transform[is.na(titanic_transform$Age), ], 10)
```

```{r}
#A estos nulos de momento se decide
#aplicar la media de la edad del resto de pasajeros
titanic_transform$Age[is.na(titanic_transform$Age)] <-
mean(titanic_transform$Age, na.rm = TRUE)
```

```{r}
#Comprobamos que ya no tenemos nulos en la edad
titanic_transform[is.na(titanic_transform$Age), ]
```

```{r}
#Visualizamos de nuevo nustro dataset
titanic_transform %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  t()
head(titanic_transform, 10)
```

**Sibp**

```{r}
summary(titanic_transform$SibSp)
unique(titanic_transform$SibSp)
hist(titanic_transform$SibSp, col = "blue", border = "black",
     xlab = "ibSp Unique Values", ylab = "Frequency")
```

**Parch**

```{r}
#Parch

summary(titanic_transform$Parch)
```

**Sex**

```{r}
#Codificamos el sexo como 0=female, 2=male
titanic_transform <- titanic_transform %>%
  mutate(Sex = replace(Sex, Sex == "female", 0))
titanic_transform <- titanic_transform %>%
  mutate(Sex = replace(Sex, Sex == "male", 1))
```

**Embarked**

```{r}
#Para la variable Embarked utilizaremos el proceso
#one-hot enconding para codificar los valores

#Analizamos los diferentes atributos para esta variable
unique(titanic_transform$Embarked)

#Creamos las variables dummy para la transformacion
titanic_onehot <- data.frame(titanic$Embarked)
dummy <- dummyVars(" ~ .", data = titanic_onehot)
titanic_one_hot2 <- data.frame(predict(dummy, newdata = titanic_onehot))

#Como tan solo hay 2 campos que tienen valor vacio
#se decide eliminar esta columna
titanic_one_hot2$titanic.Embarked <- NULL

#Se unen ambos dataframes
titanic_transform <- dplyr::bind_cols(titanic_transform, titanic_one_hot2)
#eliminamos la columna original de Embarked
titanic_transform$Embarked <- NULL
```

**PLCLASS**

```{r}
summary(titanic_transform$Pclass)
unique(titanic_transform$Pclass)
#En principio no se hace ninguna modificacion sobre
#esta columna. Se vera si en el futuro se decice
#normalizarla en caso de que sea util para
#entrenar un modelo de prediccion
```

**Cabin**

```{r}
summary(titanic_transform$Cabin)
unique(titanic_transform$Cabin)
#Mas del 80% de los campos estan vacios por lo
#que se decide eliminar esta columna ya que
#seguramente entorpezca mas el entrenamiento
#con tan pocos atributos
titanic_transform$Cabin <- NULL
```

#### Normalizacion de variables

```{r}
#Creamos una funcion para normalizar
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}

#normalizamos la columna edad
titanic_transform$Age_norm <- normalize(titanic_transform$Age)

#eliminamos la columna Age original dejando solo la normalizada
titanic_transform$Age <- NULL
```

```{r}
# Normalizamos Fare pero antes quitamos los na que tiene 1
titanic_transform <- titanic_transform[!is.na(titanic_transform$Fare), ]
titanic_transform$Fare_norm <- normalize(titanic_transform$Fare)

#eliminamos la columna head original dejando solo la normalizada
titanic_transform$Fare <- NULL
head(titanic_transform, 10)
```

#### Algunas visualizaciones en exploracion de datos

#### Ver relación entre el sexo y los supervivientes

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = titanic_transform[], aes(x = Sex, fill = Survived)) + geom_bar()
```

#### Ver relación entre la puerta de embarque y los supervivientes

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = titanic[], aes(x = Embarked, fill = Survived)) + geom_bar()
```

#### Ver relación entre la clase y los supervivientes

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = titanic[], aes(x = Pclass, fill = Survived)) + geom_bar()
```

#### Conclusiones

Se ha realizado una exploración y transformacion general de las variables oportunas. En caso de derivar el dataset para un proceso de aprendizaje automático se seguiría analizando si es necesario transformas más varibales(normalización, codificación, eliminación, etc.).

### Ejercicio 2: Presentación del paquete TM

### A continuación, se presenta el paquete TM para minería de Texto.

#### 1 - Comenzamos con la instalación y carga de los paquetes necesarios: tm y wordcloud.

```{r}
install.packages("tm")
library(tm)

install.packages("wordcloud")
library(wordcloud)

install.packages("ggplot2")
library(ggplot2)
```

#### 2 - Vamos a generar un corpus con los documentos que se van a analizar y transformar:

Los documentos a analizar tendrán formato PDF y deben estar en el directorio ./papers desde el que se ejecuta este Notebook.

```{r}
getwd()

#Los documentos (pdf) deben estar en el directorio ./papers

# Generamos una lista con lo nombres de los ficheros pdf en ./papers

papers <- list.files(path = "./papers", pattern = "pdf$")
papers <- lapply(papers, paste)

#Comprobamos que detectan los documentos

papers <- paste("./papers/", papers, sep = "")
```

Podemos ver la lista de documentos que se van a incorporar al corpus.

```{r}
papers
```

En el siguiente paso se crea el corpus incorporando los documentos pdf existentes en el directorio.

Los documentos se transforman en texto plano.

```{r}
#procedemos a crear el corpus con los documentos en texto obtenidos de los pdfs

corp <- Corpus(URISource(papers),
              readerControl = list(reader = readPDF))
```

Mostramos el contenido del corpus y las características de cada documento:

```{r}
#Inspeccionamos el contenido del corpus mostrando para cada documento
#número de metadatos y el número de carácteres de cada doc
inspect(corp)
```

Podemos visualizar los metadatos de cada documento recorriendo el corpus con un bucle for:

```{r}
#Visualizamos los metadatos de cada uno de los documentos
for (i in seq_along(corp)) {
  print(meta(corp[[i]]))
}

dtm <- DocumentTermMatrix(corp)

```

#### 3 - Comenzamos con el análisis de los documentos.

Primero vamos a generar la mátriz del corpus **(DocumentTermMatrix)** y las estadísticas de esta:

-   Número de documentos

-   Número de términos.

-   Dispersión.

-   Máxima longitud o máximo número de carácteres para un término.

```{r}
#Creamos la Mátriz de términos
dtm <- DocumentTermMatrix(corp)

#Visualizamos las estádisticas
dtm

```

#### 4 - Vamos a transformar los documentos del corpus para realizar un mejor análisis.

Se va a reducir el número de términos mediante el siguiente proceso:

1.  Eliminamos los números ya que se consideran términos y no van a ser objeto del análisis.

2.  Quitamos los signos de puntuación.

3.  Se convierte el texto a minúsculas ya que TM es sensible a las mayúsculas.

4.  Eliminamos las palabras sin significado relevante para el análisis: preposiciones, conjunciones (stopwords).

5.  Eliminamos espacios redundantes

```{r}
#Para reducir el volumen de términos y quedarnos solo con las palabras
#clave:

#1) Eliminamos los números (no son objeto del análisis)
corp <- tm_map(corp, removeNumbers)

#2) eliminamos los signos de puntuación
corp <- tm_map(corp, removePunctuation)

#pasamos el texto a minúsculas
corp <- tm_map(corp, content_transformer(tolower))

#Eliminamos las palabras comunes como artículo y preposiciones en inglés
corp <- tm_map(corp, removeWords, stopwords(kind = "en"))

#Eliminamos espacios consecutivos
corp <- tm_map(corp, stripWhitespace)
```

Volvemos a visualizar las estadísticas: El número de términos y la dispersión han debido reducirse al eliminar los elementos anteriores.

```{r}
#Volvemos a generar la Matriz de términos y visualizamos estadísticas
dtm <- DocumentTermMatrix(corp)
dtm
```

Presentamos el histograma de términos más frecuentes en el corpus:

```{r}
frec <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
df <- data.frame(terminos = names(frec), ocurrencia = frec)

p <- ggplot(subset(df, frec > 70), aes(terminos, ocurrencia))
p <- p + scale_fill_brewer(palette = "Blues")
p <- p + geom_bar(stat = "identity", color = "blue", fill = "white")
p <- p + theme(axis.text.x = element_text(angle = 45, hjust = 1))
p
```

#### 5 - Vamos a representar la nube de palabras del corpus:

Para ello utilizaremos una nube de palabras generada por la librería **wordcloud**

El proceso es el siguiente:

1.  Calculamos para cada término de la matriz la suma total de apariciones (para todo el corpus).

2.  Se ordena de forma de mayor a menor frecuencia.

3.  Se selecciona un número máximo de palabras (30 en este caso)

4.  Se dibuja la nube de palabras mediante la función wordcloud.

```{r}
set.seed(16)
wordcloud(names(frec), frec, min.freq = 10, max.words = 30
      , colors = brewer.pal(8, "Dark2"))
```

### Ejercicio 3: Estudio y presentación de dataset cáncer

**Cargamos los datos del dataset cáncer**

```{r}
df_cancer <- fread(paste0(data_link, "Breast_Cancer.csv"), sep = ",", dec = ".")
head(df_cancer, 10)
```

**Vemos la estructura del dataframe**

```{r}
str(df_cancer)
```

**Resumen de las caracteristicas de las variables del dataframe**

```{r}
summary(df_cancer)
```

**Vemos si el dataframe tiene Na**

```{r}
# Exploramos las primeras 10 lineas del df para ver si hay algún TRUE
head(is.na(df_cancer), 10)
# Vemos la posición en la que hay nulls, si los hay
print("-------------------------------------------------------")
apply(is.na(df_cancer), 2, which) # No tiene Na
```

**Obtenemos los valores únicos de las variables**

```{r}
unique(df_cancer$Age)
unique(df_cancer$Race)
unique(df_cancer$`Marital Status`)
unique(df_cancer$`T Stage`)
unique(df_cancer$`N Stage`)
unique(df_cancer$`6th Stage`)
unique(df_cancer$differentiate)
unique(df_cancer$Grade)
unique(df_cancer$`A Stage`)
unique(df_cancer$`Tumor Size`)
unique(df_cancer$`Estrogen Status`)
unique(df_cancer$`Progesterone Status`)
unique(df_cancer$`Regional Node Examined`)
unique(df_cancer$`Reginol Node Positive`)
unique(df_cancer$`Survival Months`)
unique(df_cancer$Status)
```

**Se reemplaza el valor del grado desacorde por 4**

```{r}
df_cancer$Grade[df_cancer$Grade == "anaplastic; Grade IV"] <- "4"
df_cancer$Grade <- as.integer(df_cancer$Grade)
```

**Vemos si el dataframe tiene datos duplicados**

```{r}
# El df tiene una fila duplicada
df_cancer[duplicated(df_cancer)]
# Borramos la fila duplicada
head(df_cancer[!duplicated(df_cancer)], 10)
```

**Cambiar el valor de la variable Status a numerico**

```{r}
# Si sobrevive asignamos un 1 y si se muere es un 0
df_cancer$Status <- c(Alive = 1, Dead = 0)[df_cancer$Status]
unique(df_cancer$Status)
head(df_cancer, 10)
```

**Conteo de variables**

```{r}
# Conteo para cada raza y grado
table(df_cancer$Race, df_cancer$Grade)
```

Se puede apreciar que la raza blanca tiene sobretodo cáncer en grado 2

```{r}
# Conteo para cada estado marital y los meses de supervivencia
table(df_cancer$`Marital Status`, df_cancer$Grade)
```

Se sigue apreciando que los que tienen un conteo mayor son los casados en estadio 2

```{r}
base <- df_cancer[, c("Age", "Grade", "Tumor Size", "Regional Node Examined",
                      "Reginol Node Positive", "Survival Months", "Status")]
correlacion <- round(cor(base), 1)
corrplot(correlacion, method = "number", type = "upper")
```

```{r}
cor(df_cancer$Age, df_cancer$Status, method =
      c("pearson", "kendall", "spearman"))
cor(df_cancer$`Tumor Size`, df_cancer$Status, method =
      c("pearson", "kendall", "spearman"))
```

```{r}
df_cancer %>% ggplot(aes(Status, fill = Status)) + geom_bar()
a <- df_cancer %>% ggplot(aes(Age, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
b <- df_cancer %>% ggplot(aes(Race, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
c <- df_cancer %>% ggplot(aes(`Marital Status`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
d <- df_cancer %>% ggplot(aes(`T Stage`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
e <- df_cancer %>% ggplot(aes(`N Stage`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
f <- df_cancer %>% ggplot(aes(`6th Stage`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
g <- df_cancer %>% ggplot(aes(differentiate, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
h <- df_cancer %>% ggplot(aes(Grade, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
i <- df_cancer %>% ggplot(aes(`A Stage`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
j <- df_cancer %>% ggplot(aes(`Tumor Size`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
k <- df_cancer %>% ggplot(aes(`Estrogen Status`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
l <- df_cancer %>% ggplot(aes(`Progesterone Status`,fill = Status)) +
  geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
m <- df_cancer %>% ggplot(aes(`Regional Node Examined`, fill = Status)) +
  geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
n <- df_cancer %>% ggplot(aes(`Reginol Node Positive`, fill = Status)) +
  geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
o <- df_cancer %>% ggplot(aes(`Survival Months`, fill = Status)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, ncol = 2, nrow = 8)
```
